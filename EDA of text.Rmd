---
title: "EDA"
author: "Rongqian Zhang, Yinuo Chen, Keyu Chen"
date: "`r format(Sys.time(),'%m/%d/%Y')`"
output: 
  html_document:
    theme: spacelab
    highlight: tango
    includes:
    toc: true
    number_sections: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_folding: show
vignette: >
  %\VignetteIndexEntry{EDA of text}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include=FALSE}
library(dplyr)
library(ggplot2)
library(tidytext)
library(stringr) 
library(tidyr)   
library(wordcloud)
library(reshape2)

library(hunspell)
library(SnowballC)
library(xtable)
library(knitr)
library(kableExtra)
library(scales)
```

# Preprocessing Text

```{r, include=FALSE}
text<-read.csv('merge.csv')
names(text)[names(text) == "n"]      <- "id_review"
names(text)[names(text) == "RFV"]    <- "text_review"
cleaned_text <- text %>%
  filter(str_detect(text_review, "^[^>]+[A-Za-z\\d]") | text_review !="") 
 cleaned_text$text_review <- gsub("[_]", "", cleaned_text$text_review)
 cleaned_text$text_review <- gsub("<br />", "", cleaned_text$text_review)
text_df <- tibble(id_review = cleaned_text$id_review , text_review = cleaned_text$text_review,image=cleaned_text$ANYIMAGE)

```

#  Tokenization

```{r, include=FALSE}
text_df <- text_df %>%  unnest_tokens(word, text_review)
```

# Stemming Words 

```{r, include=FALSE}
getStemLanguages() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
text_df$word <- wordStem(text_df$word,  language = "english")

head(table(text_df$word)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

```

# Stop Words

```{r, include=FALSE}
data(stop_words)
text_df <- text_df %>% 
  anti_join(stop_words, "word")


```

# Most common words

```{r, include=FALSE}
xtable(head(text_df %>% 
              count(word, sort = TRUE))) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

text_df %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 1000) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) + 
  geom_col() + 
  xlab(NULL) + 
  coord_flip()

```

# WordCloud 

```{r}
text_df %>% 
  anti_join(stop_words, "word") %>%
  count(word) %>% 
  with(wordcloud(word, n, max.words =75,scale=c(3.5,.5),random.order=FALSE))
```

# Comparison word frequency between 0 and 1 in ANYIMAGE

```{r, include=FALSE}
frequency = text_df %>% 
  group_by(image) %>% 
  count(word, sort = TRUE) %>% 
  left_join(text_df  %>% 
              group_by(image) %>% 
              summarise(total = n())) %>%
  mutate(freq = n/total)
frequency <- as.data.frame(frequency)


frequency = frequency %>% select(image, word, freq) %>% spread(image, freq) %>% arrange(0, 1)

frequency
```


```{r, include=FALSE}
ggplot(frequency, aes(`0`, `1`)) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  geom_abline(color = "red")
```

# Comparing word usage by log odds ratio

```{r, include=FALSE}
word_ratios <- text_df  %>%
  
  count(word, image) %>%
  group_by(word) %>%
  filter(sum(n) >= 10) %>%
  ungroup() %>%
  spread(image, n, fill = 0) %>%
  mutate_if(is.numeric, list(~(. + 1) / (sum(.) + 1))) %>%
  mutate(logratio = log(`0` /`1`)) %>%
  arrange(desc(logratio))
word_ratios %>% 
  arrange(abs(logratio))

word_ratios %>%
  group_by(logratio < 0) %>%
  top_n(15, abs(logratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio)) %>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  ylab("log odds ratio (`0`/`1`)") +
  scale_fill_discrete(name = "", labels = c("0", "1"))
```

# Preprocessing Text

```{r, include=FALSE}
text<-read.csv('merge.csv')
names(text)[names(text) == "CAUSE"]    <- "text_review"
names(text)[names(text) == "n"]      <- "id_review"
cleaned_text <- text %>%
  filter(str_detect(text_review, "^[^>]+[A-Za-z\\d]") | text_review !="")
cleaned_text$text_review <- gsub("[_]", "", cleaned_text$text_review)
cleaned_text$text_review <- gsub("<br />", "", cleaned_text$text_review)
text_df <- tibble(id_review = cleaned_text$id_review , text_review = cleaned_text$text_review,image=cleaned_text$ANYIMAGE)
text_df$text_review[text_df$text_review=="  "]<-NA
```

#  Tokenization

```{r, include=FALSE}
text_df <- text_df %>%  unnest_tokens(word, text_review)
```

# Stemming Words 

```{r, include=FALSE}
getStemLanguages() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
text_df$word <- wordStem(text_df$word,  language = "english")

head(table(text_df$word)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

```

# Stop Words

```{r, include=FALSE}
data(stop_words)
text_df <- text_df %>% 
  anti_join(stop_words, "word")


```

# Most common words

```{r, include=FALSE}
xtable(head(text_df %>% 
              count(word, sort = TRUE))) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)

text_df %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 1000) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) + 
  geom_col() + 
  xlab(NULL) + 
  coord_flip()

```

# WordCloud 

```{r, include=FALSE}
text_df %>% 
  anti_join(stop_words, "word") %>%
  count(word) %>% 
  with(wordcloud(word, n, max.words =75,scale=c(3.5,.5),random.order=FALSE))
```

# Comparison word frequency between 0 and 1 in ANYIMAGE

```{r, include=FALSE}
frequency <- text_df %>% 
  group_by(image) %>% 
  count(word, sort = TRUE) %>% 
  left_join(text_df  %>% 
              group_by(image) %>% 
              summarise(total = n())) %>%
  mutate(freq = n/total)

frequency <- frequency %>% 
  select(image, word, freq) %>% 
  spread(image, freq) %>%
  arrange(0, 1)

frequency
```


```{r, include=FALSE}
ggplot(frequency, aes(`0`, `1`)) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  geom_abline(color = "red")
```

# Comparing word usage by log odds ratio

```{r, include=FALSE}
word_ratios <- text_df  %>%
  
  count(word, image) %>%
  group_by(word) %>%
  filter(sum(n) >= 10) %>%
  ungroup() %>%
  spread(image, n, fill = 0) %>%
  mutate_if(is.numeric, list(~(. + 1) / (sum(.) + 1))) %>%
  mutate(logratio = log(`0` /`1`)) %>%
  arrange(desc(logratio))
word_ratios %>% 
  arrange(abs(logratio))

word_ratios %>%
  group_by(logratio < 0) %>%
  top_n(15, abs(logratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio)) %>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  ylab("log odds ratio (`0`/`1`)") +
  scale_fill_discrete(name = "", labels = c("0", "1"))
```

Term Frequency (tf) It is one measure of how important a word may be and how frenquently a word occurs in a document. Inverse Document Frequency (idf) It decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. Calculating tf-idf attemps to find the words that are important in a text, but not too common.